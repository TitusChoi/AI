{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlp_model.ipynb\n",
    "Description   : Multiple layer perceptron to solve general problems<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mathutil library 불러오기\n",
    "%run .../mathutil.ipynb\n",
    "np,random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "class Model(object):\n",
    "    def __init__(self, name, dataset):\n",
    "        self.name = name\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        # 다른 설정값이 더 있을 때\n",
    "        if not hasattr(self, 'rand_std') : self.rand_std = 0.030\n",
    "    \n",
    "    # 문자열을 초기화\n",
    "    def __str__(self):\n",
    "        return '{}/{}'.format(self.name, self.dataset)\n",
    "    \n",
    "    # main function, 훈련값, 미니배치 사이즈, 훈련율, 훈련 중간 기록 횟수, 시각화 몇번 해줄 것인지?, \n",
    "    def exec_all(self, epochs = 10, mb = 10, lr = 0.001, report = 1, show = 1):\n",
    "        self.train(epochs, mb, lr, report)\n",
    "        self.test()\n",
    "        if show > 0:\n",
    "            self.visualize(show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpModel(Model):\n",
    "    def __init__(self, name, dataset, hconfigs):\n",
    "        # class 상속을 통해 model에서 사용하는 정보를 가져옴\n",
    "        super(MlpModel, self).__init__(name, dataset)\n",
    "        self.init_parameters(hconfigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter initialization\n",
    "def mlp_init_parameters(self, hconfigs):\n",
    "    self.hconfigs = hconfigs\n",
    "    self.pm_hiddens = []\n",
    "    \n",
    "    prev_shape = self.dataset.input_shape\n",
    "    \n",
    "    for hconfig in hconfigs:\n",
    "        pm_hidden, prev_shape = self.alloc_layer_param(prev_shape, hconfig)\n",
    "        self.pm_hiddens.append(pm_hidden)\n",
    "        \n",
    "    output_cnt = int(np.prod(self.dataset.output_shape))\n",
    "    self.pm_output, _ = self.alloc_layer_param(prev_shape, output_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter layer initialization\n",
    "def mlp_alloc_layer_param(self, input_shape, hconfig):\n",
    "    input_cnt = np.prod(input_shape)\n",
    "    output_cnt = hconfig\n",
    "\n",
    "    weight, bias = self.alloc_param_pair([input_cnt, output_cnt])\n",
    "\n",
    "    return {'w':weight, 'b':bias}, output_cnt\n",
    "\n",
    "MlpModel.alloc_layer_param = mlp_alloc_layer_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight, bias initialization\n",
    "def mlp_alloc_param_pair(self, shape):\n",
    "    weight = np.random.normal(0, self.rand_std, shape)\n",
    "    bias = np.zeros([shape[-1]])\n",
    "    return weight, bias\n",
    "\n",
    "MlpModel.init_parameters = mlp_init_parameters\n",
    "MlpModel.alloc_layer_param = mlp_alloc_layer_param\n",
    "MlpModel.alloc_param_pair = mlp_alloc_param_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
